{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Silver Data Cleaning\n",
    "\n",
    "**Purpose:** Clean raw data from the [Bronze](./1_bronze.ipynb) layer to create a unified data asset. This includes column standardization, data type enforcement, value harmonization, deduplication, and provenance tracking.\n",
    "\n",
    "**Transformations Applied:**\n",
    "- **Standardize** column names to lowercase snake_case\n",
    "- **Tag** each row with its source region for provenance\n",
    "- **Harmonize** categorical values across data sources\n",
    "- **Enforce** consistent data types\n",
    "\n",
    "This data will be used when creating [Gold](./3_gold.ipynb), where tailored data assets will be created to efficiently answer specific questions.\n",
    "\n",
    "\n",
    "For more on Medallion Architecture, see [Databricks Glossary: Medallion Architecture](https://www.databricks.com/glossary/medallion-architecture) (Databricks, n.d.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Setup](#1-setup)  \n",
    "   Install required packages and import libraries.\n",
    "\n",
    "1. [Configuration & Data Loading](#2-configuration-and-data-loading)  \n",
    "   Centralize file paths, API parameters, and date-column lists, then ingest the raw Bronze dataset into pandas.\n",
    "\n",
    "1. [Define Helper Functions](#3-define-helper-functions)  \n",
    "   Define all cleaning and enrichment transforms as modular functions—date anomaly filters, age parsers, imputation routines, etc.\n",
    "\n",
    "1. [Data Cleaning & Standardization](#4-data-cleaning--standardization)  \n",
    "   Harmonize column names, drop duplicates, and enforce schema across sources.\n",
    "\n",
    "1. [Value Mapping & Data Type Enforcement](#5-value-mapping--data-type-enforcement)  \n",
    "   Apply categorical/value mappings and cast explicit dtypes for Silver.\n",
    "\n",
    "1. [Execute Transformations](#6-execute-transformations)  \n",
    "   Run each helper function in sequence to clean and enrich the DataFrame.\n",
    "\n",
    "1. [Create Silver](#7-create-silver)  \n",
    "   Inspect missingness, distributions, date ranges, and trends to validate Silver.\n",
    "\n",
    "1. [Post-Processing of the Silver Data](#8-post-processing-of-the-silver-data)\n",
    "\n",
    "    8.1. [Compute Age from Intake and DOB](#81-compute-age-from-intake-and-dob)\n",
    "\n",
    "    8.2. [Apply species-specific median imputations for missing ages](#82-apply-species-specific-median-imputations-for-missing-ages)\n",
    "\n",
    "    8.3. [Bin Ages into 3 Life Stages (Puppy/Kitten, Adult, Senior)](#83-bin-ages-into-3-life-stages-puppykitten-adult-senior)\n",
    "\n",
    "    8.4. [Extract Sex from Health, Apply Imputation for Missing Sex](#84-extract-sex-from-health-apply-imputation-for-missing-sex)\n",
    "\n",
    "    8.5. [Apply Imputation to Assign Sex Based on Group Distribution](#85-apply-imputation-to-assign-sex-based-on-group-distribution)\n",
    "\n",
    "    8.6. [Apply Imputation to Assign Primary Color for Missing Values](#86-apply-imputation-to-assign-primary-color-for-missing-values) \n",
    "\n",
    "1. [Materialize Silver](#9-materialize-silver)\n",
    "   Add in description\n",
    "\n",
    "1. [Data Quality Assessment](#10-data-quality-assessment)\n",
    "   Add in description\n",
    "\n",
    "1. [References](#11-references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "**Purpose:**  \n",
    "Ensure the environment has all necessary libraries installed and imported.  \n",
    "```python\n",
    "# Install project-wide dependencies\n",
    "%pip install -r ../../requirements.txt\n",
    "``` \n",
    "\n",
    "> **Note:** we use a project-wide `requirements.txt` for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn==0.13.2 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages (from -r ../../requirements.txt (line 1)) (0.13.2)\n",
      "Requirement already satisfied: pandas==2.2.2 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages (from -r ../../requirements.txt (line 2)) (2.2.2)\n",
      "Requirement already satisfied: matplotlib==3.9.2 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages (from -r ../../requirements.txt (line 3)) (3.9.2)\n",
      "Requirement already satisfied: pyarrow==20.0.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages (from -r ../../requirements.txt (line 4)) (20.0.0)\n",
      "Requirement already satisfied: numpy==1.26.4 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages (from -r ../../requirements.txt (line 5)) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages (from pandas==2.2.2->-r ../../requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages (from pandas==2.2.2->-r ../../requirements.txt (line 2)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages (from pandas==2.2.2->-r ../../requirements.txt (line 2)) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages (from matplotlib==3.9.2->-r ../../requirements.txt (line 3)) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages (from matplotlib==3.9.2->-r ../../requirements.txt (line 3)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages (from matplotlib==3.9.2->-r ../../requirements.txt (line 3)) (4.58.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages (from matplotlib==3.9.2->-r ../../requirements.txt (line 3)) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages (from matplotlib==3.9.2->-r ../../requirements.txt (line 3)) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages (from matplotlib==3.9.2->-r ../../requirements.txt (line 3)) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages (from matplotlib==3.9.2->-r ../../requirements.txt (line 3)) (3.2.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas==2.2.2->-r ../../requirements.txt (line 2)) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r ../../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration and Data Loading\n",
    "\n",
    "**Purpose:**\n",
    "Here we centralize file paths, API endpoints, and date-column definitions, then ingest every raw Bronze source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data source configurations\n",
    "BRONZE_DIR = \"../../data-assets/bronze\"\n",
    "BRONZE_FILE_NAME = \"{}_df.parquet\"\n",
    "\n",
    "# Load all the Bronze datasets\n",
    "BRONZE_FILES = [\"dallas\", \"san_jose\", \"soco\"]\n",
    "BRONZE_FILE_PATHS = {\n",
    "    file: os.path.join(BRONZE_DIR, BRONZE_FILE_NAME.format(file)) for file in BRONZE_FILES \n",
    "}\n",
    "BRONZE_DFS = {\n",
    "    file: pd.read_parquet(path) for file, path in BRONZE_FILE_PATHS.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Helper Functions\n",
    "\n",
    "**Purpose:**\n",
    "Below, we define some functions to help us with our transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Data Cleaning ───\n",
    "\n",
    "# Function to apply the column mapping \n",
    "def standardize_columns(source: str, df: pd.DataFrame, mapping: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Standardize DataFrame column names using mapping, lowercase, and remove duplicates.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Raw DataFrame to standardize\n",
    "    mapping : dict\n",
    "        Column name mapping (original -> standardized)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with renamed, lowercased columns and duplicates removed\n",
    "    \"\"\"\n",
    "    # Apply the renaming mapping\n",
    "    df = df.rename(columns=mapping)\n",
    "    # Convert all column names to lowercase\n",
    "    df.columns = df.columns.str.lower()\n",
    "    # Remove duplicate columns, keeping the first occurrence\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "    print(f\" - {source}: {list(df.columns)}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def apply_silver_transforms(df: pd.DataFrame, source: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply comprehensive silver-layer transformations to a DataFrame.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame with standardized columns\n",
    "    source : str\n",
    "        Source identifier for provenance tracking\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Transformed DataFrame with harmonized values and proper types\n",
    "    \"\"\"\n",
    "    # Add provenance\n",
    "    df['region'] = source\n",
    "    \n",
    "    # Ensure intake_reason column exists\n",
    "    if 'intake_reason' not in df.columns:\n",
    "        df['intake_reason'] = pd.NA\n",
    "    \n",
    "    # Apply data types\n",
    "    for col, dtype in SILVER_DTYPES.items():\n",
    "        if col in df.columns:\n",
    "            if dtype == 'datetime64[ns]':\n",
    "                df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "            elif col == \"age\":\n",
    "            # Clean age strings like \"6 MONTHS\", \"2 YEARS\", etc.\n",
    "                age_str = df[col].astype(str).str.strip().str.upper()\n",
    "\n",
    "                extracted = age_str.str.extract(r'(?P<value>\\d+\\.?\\d*)\\s*(?P<unit>YEAR|YEARS|MONTH|MONTHS)?')\n",
    "                extracted['value'] = extracted['value'].astype(float)\n",
    "\n",
    "            # Convert months to years if unit is MONTH(S)\n",
    "                df[col] = extracted.apply(\n",
    "                    lambda row: row['value'] / 12 if row['unit'] in ['MONTH', 'MONTHS']\n",
    "                    else row['value'],\n",
    "                    axis=1\n",
    "                )\n",
    "            else:\n",
    "                df[col] = df[col].astype(dtype)\n",
    "    \n",
    "    # Data validation: Check for future dates\n",
    "    current_date = pd.Timestamp.now().normalize()\n",
    "    date_columns = ['intake_date', 'outcome_date']\n",
    "    \n",
    "    for col in date_columns:\n",
    "        if col in df.columns:\n",
    "            future_dates = df[col] > current_date\n",
    "            if future_dates.any():\n",
    "                future_count = future_dates.sum()\n",
    "                max_future_date = df.loc[future_dates, col].max()\n",
    "                print(f\"WARNING: Found {future_count:,} future dates in {col} for {source}\")\n",
    "                print(f\"         Latest future date: {max_future_date.date()}\")\n",
    "                print(f\"         Setting future dates to NaT (Not a Time)\")\n",
    "                \n",
    "                # Set future dates to NaT\n",
    "                df.loc[future_dates, col] = pd.NaT\n",
    "    \n",
    "    # Harmonize categorical values\n",
    "    for col, mapping in VALUE_MAPPINGS.items():\n",
    "        if col in df.columns:\n",
    "            # Normalize text before mapping\n",
    "            normalized = df[col].astype(str).str.strip().str.upper()\n",
    "            df[col] = normalized.map(mapping).fillna('other' if col != 'intake_reason' else 'unknown')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_silver_dataset(dataframes: dict[str, pd.DataFrame], schema: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Combine multiple source DataFrames into unified silver dataset.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframes : dict[str, pd.DataFrame]\n",
    "        Source DataFrames to combine\n",
    "    schema : list[str]\n",
    "        Final column schema to enforce\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Unified silver dataset\n",
    "    \"\"\"\n",
    "    # Combine all sources\n",
    "    combined = pd.concat(dataframes.values(), ignore_index=True, sort=False)\n",
    "    \n",
    "    # Enforce schema\n",
    "    return (\n",
    "        combined\n",
    "        .reindex(columns=schema)\n",
    "        # .drop_duplicates() Dropping duplicates may miss repeat intakes TBD\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "\n",
    "def create_cardinality_summary(dataframes: dict, columns: list) -> pd.DataFrame:\n",
    "    \"\"\"Create cardinality summary showing unique value counts by source and column.\"\"\"\n",
    "    \n",
    "    summary = (\n",
    "        pd.DataFrame({\n",
    "            source: {\n",
    "                col: df[col].nunique(dropna=True)  # Count unique non-null values\n",
    "                for col in columns if col in df     # Skip missing columns\n",
    "            }\n",
    "            for source, df in dataframes.items()   # Process each data source\n",
    "        })\n",
    "        .fillna(0)      # Missing columns = 0 unique values\n",
    "        .astype(int)    # Convert to clean integers\n",
    "        .T              # Transpose: sources as rows, columns as features\n",
    "    )\n",
    "    \n",
    "    # Return with integer formatting\n",
    "    return summary.style.format(\"{:d}\")\n",
    "\n",
    "def generate_data_overview(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Generate comprehensive data quality overview.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Dataset to profile\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DATA QUALITY PROFILE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Dataset overview\n",
    "    print(f\"\\nDATASET OVERVIEW\")\n",
    "    print(f\"Total records: {df.shape[0]:,}\")\n",
    "    print(f\"Total columns: {df.shape[1]}\")\n",
    "    \n",
    "    # Missing data analysis\n",
    "    print(f\"\\nMISSING DATA ANALYSIS\")\n",
    "    missing_data = df.isnull().sum()\n",
    "    missing_pct = (missing_data / len(df) * 100).round(3)\n",
    "    \n",
    "    for col in missing_data.index:\n",
    "        if missing_data[col] > 0:\n",
    "            # change missing_pcft to .4f\n",
    "            print(f\"  {col}: {missing_data[col]:,} ({missing_pct[col]:.3f}%)\")\n",
    "    \n",
    "    # Cardinality analysis\n",
    "    print(f\"\\nCARDINALITY ANALYSIS\")\n",
    "    cardinality = df.nunique().sort_values(ascending=False)\n",
    "    for col, count in cardinality.items():\n",
    "        print(f\"  {col}: {count:,} unique values\")\n",
    "    \n",
    "    # Categorical distributions\n",
    "    categorical_cols = ['intake_type'\n",
    "                        , 'animal_type'\n",
    "                        , 'breed'\n",
    "                        , 'primary_color'\n",
    "                        , 'intake_condition'\n",
    "                        , 'intake_reason'\n",
    "                        , 'outcome_type'\n",
    "                        , 'animal_type']\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if col in df.columns:\n",
    "            print(f\"\\n{col.upper()} DISTRIBUTION\")\n",
    "            dist = df[col].value_counts(normalize=True).head(10)\n",
    "            for value, pct in dist.items():\n",
    "                print(f\"  {value}: {pct:.1%}\")\n",
    "    \n",
    "    # Temporal analysis\n",
    "    print(f\"\\nTEMPORAL ANALYSIS\")\n",
    "    if 'intake_date' in df.columns:\n",
    "        date_range = df['intake_date'].agg(['min', 'max'])\n",
    "        print(f\"  Intake date range: {date_range['min'].date()} to {date_range['max'].date()}\")\n",
    "        \n",
    "        # Monthly trends\n",
    "        monthly = df.set_index('intake_date').resample('M').size()\n",
    "        print(f\"  Average monthly intake: {monthly.mean():.0f} animals\")\n",
    "        print(f\"  Peak month: {monthly.idxmax().strftime('%B %Y')} ({monthly.max():,} animals)\")\n",
    "\n",
    "\n",
    "def compute_age_from_dates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute the age from dates intake date and date of birth\n",
    "    \"\"\"\n",
    "    if {\"age\", \"intake_date\", \"date_of_birth\"}.issubset(df.columns):\n",
    "        # Convert date columns to datetime if not already\n",
    "        df[\"intake_date\"] = pd.to_datetime(df[\"intake_date\"], errors='coerce')\n",
    "        df[\"date_of_birth\"] = pd.to_datetime(df[\"date_of_birth\"], errors='coerce')\n",
    "\n",
    "        # Create a mask for the rows where age is missing but both intake_date and date_of_birth are available\n",
    "        mask = df[\"age\"].isna() & df[\"intake_date\"].notna() & df[\"date_of_birth\"].notna()\n",
    "        \n",
    "        df.loc[mask, \"age\"] = (df.loc[mask, \"intake_date\"] - df.loc[mask, \"date_of_birth\"]).dt.days / 365.25\n",
    "        print(f\"Computed age for {mask.sum()} rows\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def impute_missing_age(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply Imputation (missing age using species - specific median)\n",
    "    \"\"\"\n",
    "    if \"animal_type\" in df.columns and \"age\" in df.columns:\n",
    "        for species in df[\"animal_type\"].dropna().unique():\n",
    "            species_mask = df[\"animal_type\"] == species\n",
    "            median_age = df.loc[species_mask, \"age\"].median()\n",
    "            missing_mask = species_mask & df[\"age\"].isna()\n",
    "            df.loc[missing_mask, \"age\"] = median_age\n",
    "            print(f\"Imputed {missing_mask.sum()} missing ages for species: {species} (median={median_age:.2f})\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def bin_age_into_life_stages(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Bin ages in accordance to life stage (Puppy/Kitten, adult, senior)\n",
    "    \"\"\"\n",
    "    def categorize(row):\n",
    "        if pd.isna(row[\"age\"]) or pd.isna(row[\"animal_type\"]):\n",
    "            return pd.NA\n",
    "        if row[\"age\"] < 0.5:\n",
    "            return \"puppy\" if row[\"animal_type\"] == \"dog\" else (\n",
    "                   \"kitten\" if row[\"animal_type\"] == \"cat\" else pd.NA)\n",
    "        elif row[\"age\"] < 7:\n",
    "            return \"adult\"\n",
    "        else:\n",
    "            return \"senior\"\n",
    "    df[\"age_stage\"] = df.apply(categorize, axis=1).astype(\"category\")\n",
    "    print(\"Added 'age_stage' column with categories: puppy/kitten, adult, senior\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def recatogarize_sex(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Harmonizing male and female\n",
    "    \"\"\"\n",
    "    male_terms = ['MALE', 'Male', 'NEUTERED', 'Neutered']\n",
    "    female_terms = ['FEMALE', 'Female', 'SPAYED', 'Spayed']\n",
    "    \n",
    "    df[\"sex\"] = df[\"sex\"].apply(\n",
    "        lambda x: \"male\" if x in male_terms else\n",
    "                  \"female\" if x in female_terms else pd.NA\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def impute_sex_by_species_and_breed(df: pd.DataFrame, seed: int = 42) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Populating missing columns based on statistical distribution.\n",
    "    Logic is, I would like to observe the distribution of two key values when assigning \"sex\" for missing values : 1_animal_type , 2_breed. The logic is\n",
    "    for each row that is missing \"sex\", a compiled probability dictionary is referenced for a specific group based on the aformentioned key values. \n",
    "    using numpy random seed to assign sex based on probability distribution for each group.\n",
    "    \"\"\"\n",
    "    # Set seed for reproducibility - 42 because its the most popular number \n",
    "    np.random.seed(seed)\n",
    "    # Get normalized sex distributions per (animal_type, breed)\n",
    "    sex_probs = (\n",
    "        df.dropna(subset=[\"sex\"])\n",
    "        .groupby([\"animal_type\", \"breed\"])[\"sex\"]\n",
    "        .value_counts(normalize=True)\n",
    "        .unstack()\n",
    "        .fillna(0)\n",
    "    )\n",
    "\n",
    "    # Convert to lookup dictionary for speed\n",
    "    sex_prob_dict = sex_probs.to_dict(orient=\"index\")\n",
    "\n",
    "    def sample_sex(row):\n",
    "        \"\"\"The sampling and random logic based on distribution of sex (male/female)\"\"\"\n",
    "        if pd.notna(row[\"sex\"]):\n",
    "            return row[\"sex\"]\n",
    "        key = (row[\"animal_type\"], row[\"breed\"])\n",
    "        p = sex_prob_dict.get(key)\n",
    "        if p and (p.get(\"male\", 0) + p.get(\"female\", 0)) > 0:\n",
    "            return np.random.choice([\"male\", \"female\"], p=[p.get(\"male\", 0), p.get(\"female\", 0)])\n",
    "        return pd.NA\n",
    "\n",
    "    df[\"sex\"] = df.apply(sample_sex, axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def impute_primary_color_by_species_and_breed(df: pd.DataFrame, seed: int = 42) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Utilizing the same logic used for sex, we are able to do the same for primary color missing values\n",
    "    By going through all the rows and grouping them according to species & breed while making note of the color associated with each pair\n",
    "    We can assign a probability to each color, compiling a color distribution for each species-breed pair.\n",
    "    hence, randomly assigning a color using those accumulated probabilities for each missing row.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Compute normalized primary_color probabilities per group\n",
    "    color_probs = (\n",
    "        df.dropna(subset=[\"primary_color\"])\n",
    "        .groupby([\"animal_type\", \"breed\"])[\"primary_color\"]\n",
    "        .value_counts(normalize=True)\n",
    "        .unstack()\n",
    "        .fillna(0)\n",
    "    )\n",
    "\n",
    "    # Convert to lookup dictionary\n",
    "    color_prob_dict = color_probs.to_dict(orient=\"index\")\n",
    "    # The sampling and random logic based on distribution of primary color (species/breed to assign color)\n",
    "    def sample_color(row):\n",
    "        if pd.notna(row[\"primary_color\"]):\n",
    "            return row[\"primary_color\"]\n",
    "        key = (row[\"animal_type\"], row[\"breed\"])\n",
    "        p = color_prob_dict.get(key)\n",
    "        if p and sum(p.values()) > 0:\n",
    "            choices = list(p.keys())\n",
    "            probabilities = list(p.values())\n",
    "            return np.random.choice(choices, p=probabilities)\n",
    "        return pd.NA\n",
    "\n",
    "    df[\"primary_color\"] = df.apply(sample_color, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Cleaning & Standardization\n",
    "\n",
    "**Purpose:**  \n",
    "Align all of our sources to a common schema.\n",
    "\n",
    "> **Note:** This step enforces snake_case naming and removes accidental duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will define the full column mapping for all the DataFrames:\n",
    "COLUMN_MAP = {\n",
    "    # Animal identification\n",
    "    **{col: \"animal_id\" for col in [\"AnimalID\", \"Animal_Id\", \"Animal ID\"]}, # Using Python's dictionary operators for cleaner code!\n",
    "    **{col: \"animal_type\" for col in [\"AnimalType\", \"Animal_Type\", \"Type\"]},\n",
    "    \n",
    "    # Animal characteristics\n",
    "    **{col: \"breed\" for col in [\"PrimaryBreed\", \"Animal_Breed\", \"Breed\"]},\n",
    "    **{col: \"primary_color\" for col in [\"PrimaryColor\", \"Color\"]},\n",
    "    \"Age\": \"age\",\n",
    "    \"Date Of Birth\": \"date_of_birth\",\n",
    "    \"Sex\": \"sex\",\n",
    "    \n",
    "    # Intake information\n",
    "    **{col: \"intake_type\" for col in [\"IntakeType\", \"Intake_type\", \"Intake Type\"]},\n",
    "    **{col: \"intake_condition\" for col in [\"IntakeCondition\", \"Intake_Condition\", \"Intake Condition\"]},\n",
    "    **{col: \"intake_reason\" for col in [\"IntakeReason\", \"Reason\"]},\n",
    "    **{col: \"intake_date\" for col in [\"IntakeDate\", \"Intake_Date\", \"Intake Date\"]},\n",
    "    \n",
    "    # Outcome information\n",
    "    **{col: \"outcome_type\" for col in [\"OutcomeType\", \"outcome_type\", \"Outcome Type\"]},\n",
    "    **{col: \"outcome_date\" for col in [\"OutcomeDate\", \"Outcome_Date\", \"Outcome Date\"]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column standardization starting...\n",
      "\n",
      " - dallas: ['animal_id', 'animal_type', 'breed', 'kennel_number', 'kennel_status', 'tag_type', 'activity_number', 'activity_sequence', 'source_id', 'census_tract', 'council_district', 'intake_type', 'intake_subtype', 'intake_total', 'intake_reason', 'staff_id', 'intake_date', 'intake_time', 'due_out', 'intake_condition', 'hold_request', 'outcome_type', 'outcome_subtype', 'outcome_date', 'outcome_time', 'receipt_number', 'impound_number', 'service_request_number', 'outcome_condition', 'chip_status', 'animal_origin', 'additional_information', 'month', 'year']\n",
      " - san_jose: ['_id', 'animal_id', 'animalname', 'animal_type', 'primary_color', 'secondarycolor', 'breed', 'sex', 'dob', 'age', 'intake_date', 'intake_condition', 'intake_type', 'intakesubtype', 'intake_reason', 'outcome_date', 'outcome_type', 'outcomesubtype', 'outcomecondition', 'crossing', 'jurisdiction', 'lastupdate']\n",
      " - soco: ['name', 'animal_type', 'breed', 'primary_color', 'sex', 'size', 'date_of_birth', 'impound number', 'kennel number', 'animal_id', 'intake_date', 'outcome_date', 'days in shelter', 'intake_type', 'intake subtype', 'outcome_type', 'outcome subtype', 'intake_condition', 'outcome condition', 'intake jurisdiction', 'outcome jurisdiction', 'outcome zip code', 'location', 'count']\n",
      "\n",
      "---\n",
      "Column standardization complete.\n"
     ]
    }
   ],
   "source": [
    "# Apply standardization (renaming and lowercasing) to all DataFrames\n",
    "print(\"Column standardization starting...\\n\")\n",
    "CLEAN_DFS = {\n",
    "    source: standardize_columns(source, df, COLUMN_MAP)\n",
    "    for source, df in BRONZE_DFS.items()\n",
    "}\n",
    "print(\"\\n---\\nColumn standardization complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cardinality Analysis: Unique Value Counts by Source\n",
    "\n",
    "Below we examine how many unique values exist in each categorical column across data sources to identify inconsistencies before harmonization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE VALUE HARMONIZATION:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_8c132\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_8c132_level0_col0\" class=\"col_heading level0 col0\" >animal_type</th>\n",
       "      <th id=\"T_8c132_level0_col1\" class=\"col_heading level0 col1\" >breed</th>\n",
       "      <th id=\"T_8c132_level0_col2\" class=\"col_heading level0 col2\" >intake_type</th>\n",
       "      <th id=\"T_8c132_level0_col3\" class=\"col_heading level0 col3\" >intake_condition</th>\n",
       "      <th id=\"T_8c132_level0_col4\" class=\"col_heading level0 col4\" >intake_reason</th>\n",
       "      <th id=\"T_8c132_level0_col5\" class=\"col_heading level0 col5\" >outcome_type</th>\n",
       "      <th id=\"T_8c132_level0_col6\" class=\"col_heading level0 col6\" >primary_color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_8c132_level0_row0\" class=\"row_heading level0 row0\" >dallas</th>\n",
       "      <td id=\"T_8c132_row0_col0\" class=\"data row0 col0\" >5</td>\n",
       "      <td id=\"T_8c132_row0_col1\" class=\"data row0 col1\" >277</td>\n",
       "      <td id=\"T_8c132_row0_col2\" class=\"data row0 col2\" >11</td>\n",
       "      <td id=\"T_8c132_row0_col3\" class=\"data row0 col3\" >9</td>\n",
       "      <td id=\"T_8c132_row0_col4\" class=\"data row0 col4\" >26</td>\n",
       "      <td id=\"T_8c132_row0_col5\" class=\"data row0 col5\" >16</td>\n",
       "      <td id=\"T_8c132_row0_col6\" class=\"data row0 col6\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8c132_level0_row1\" class=\"row_heading level0 row1\" >san_jose</th>\n",
       "      <td id=\"T_8c132_row1_col0\" class=\"data row1 col0\" >5</td>\n",
       "      <td id=\"T_8c132_row1_col1\" class=\"data row1 col1\" >233</td>\n",
       "      <td id=\"T_8c132_row1_col2\" class=\"data row1 col2\" >12</td>\n",
       "      <td id=\"T_8c132_row1_col3\" class=\"data row1 col3\" >13</td>\n",
       "      <td id=\"T_8c132_row1_col4\" class=\"data row1 col4\" >27</td>\n",
       "      <td id=\"T_8c132_row1_col5\" class=\"data row1 col5\" >15</td>\n",
       "      <td id=\"T_8c132_row1_col6\" class=\"data row1 col6\" >57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8c132_level0_row2\" class=\"row_heading level0 row2\" >soco</th>\n",
       "      <td id=\"T_8c132_row2_col0\" class=\"data row2 col0\" >3</td>\n",
       "      <td id=\"T_8c132_row2_col1\" class=\"data row2 col1\" >1128</td>\n",
       "      <td id=\"T_8c132_row2_col2\" class=\"data row2 col2\" >8</td>\n",
       "      <td id=\"T_8c132_row2_col3\" class=\"data row2 col3\" >5</td>\n",
       "      <td id=\"T_8c132_row2_col4\" class=\"data row2 col4\" >0</td>\n",
       "      <td id=\"T_8c132_row2_col5\" class=\"data row2 col5\" >9</td>\n",
       "      <td id=\"T_8c132_row2_col6\" class=\"data row2 col6\" >365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x13c2b4ed0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_cols = ['animal_type'\n",
    "                    , 'breed'\n",
    "                    , 'primary_color'\n",
    "                    , 'intake_type'\n",
    "                    , 'intake_condition'\n",
    "                    , 'intake_reason'\n",
    "                    , 'outcome_type']\n",
    "\n",
    "# Lets take a peak at the cardinality of the categorical columns before harmonization\n",
    "print(\"BEFORE VALUE HARMONIZATION:\")\n",
    "create_cardinality_summary(CLEAN_DFS, categorical_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Findings:` \n",
    "\n",
    "There is high cardinality for many of the features, below we go through all of these and bucket similar categories to minimize the noise, standardize inconsistent values, and create more meaningful groupings for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Value Mapping & Data Type Enforcement\n",
    "\n",
    "**Purpose:**  \n",
    "Convert raw categorical codes into clean, analysis-ready categories and cast explicit dtypes.  \n",
    "\n",
    "> **Note:** Using `category` dtype optimizes memory and speeds up grouping operations.\n",
    "\n",
    "There is high cardinality for many of the features, below we go through all of these and bucket similar categories to minimize the noise, standardize inconsistent values, and create more meaningful groupings for analysis.\n",
    "\n",
    "**Before/After Example:**\n",
    "- **Before**: \"OWNER SUR\", \"APP SICK\", \"CHOCOLATE/TABBY\"  \n",
    "- **After**: \"surrender\", \"medical\", \"brown_tabby\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────────\n",
    "# SILVER DTYPE MAPPING\n",
    "# ──────────────────────────────────────────────────────────────────\n",
    "# Define explicit pandas dtypes for key columns\n",
    "SILVER_DTYPES = {\n",
    "    'animal_id'        : 'category',\n",
    "    'animal_type'      : 'category',\n",
    "    'breed'            : 'category',\n",
    "    'primary_color'    : 'category',\n",
    "    'age'              : 'float',\n",
    "    'sex'              : 'category',\n",
    "    'intake_type'      : 'category',\n",
    "    'intake_condition' : 'category',\n",
    "    'intake_reason'    : 'object',\n",
    "    'intake_date'      : 'datetime64[ns]',\n",
    "    'outcome_type'     : 'category',\n",
    "    'outcome_date'     : 'datetime64[ns]',\n",
    "    'region'           : 'category'\n",
    "}\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────\n",
    "# VALUE MAPPINGS\n",
    "# ──────────────────────────────────────────────────────────────────\n",
    "\n",
    "# Animal type mapping\n",
    "# Focus will be on dogs and cats, all other species will be labeled as \"other\"\n",
    "ANIMAL_TYPE_MAP = {\n",
    "    \"DOG\"              : \"dog\",                   # SO, DA, SJ\n",
    "    \"CAT\"              : \"cat\",                   # SO, DA, SJ\n",
    "    \"BIRD\"             : \"other\",                 # SO, DA, SJ\n",
    "    \"LIVESTOCK\"        : \"other\",                 # SO, DA, SJ\n",
    "    \"WILDLIFE\"         : \"other\",                 # SO, DA, SJ\n",
    "    \"OTHER\"            : \"other\"                  # SO, DA, SJ\n",
    "}\n",
    "\n",
    "# Breed mapping\n",
    "# The logic is everything that has a single breed is maintained, anything that has \n",
    "# indication of dual breed or mix will be classified as mixed [all entries with '/' or 'MIX']\n",
    "# Cats are classified according to short hair, medium hair, and long hair within the \"breed\" column.\n",
    "BREED_MAP = {\n",
    "    # ── MIXED ──\n",
    "    \"ABYSSINIAN/DOMESTIC SH\"   : \"mixed\",        # SO\n",
    "    \"ABYSSINIAN/MIX\"           : \"mixed\",        # SO\n",
    "    \"AFFENPINSCHER/MIX\"        : \"mixed\",        # SO\n",
    "    \"ALASKAN HUSKY/LABRADOR RETR\" : \"mixed\",     # SO\n",
    "    \"GERM SHEPHERD/CHOW CHOW\"  : \"mixed\",        # SO\n",
    "    \"LABRADOR RETR/MIX\"        : \"mixed\",        # SO\n",
    "    \"PIT BULL/MIX\"             : \"mixed\",        # SO\n",
    "    \n",
    "    # ── PIT BULL ──\n",
    "    \"PIT BULL\"                 : \"pit_bull\",     # DA\n",
    "    \"AM PIT BULL TER\"          : \"pit_bull\",     # SO\n",
    "    \n",
    "    # ── LABRADOR ──\n",
    "    \"LABRADOR RETR\"            : \"labrador\",     # SO, DA\n",
    "    \"LAB\"                      : \"labrador\",     # SJ\n",
    "    \n",
    "    # ── GERMAN SHEPHERD ──\n",
    "    \"GERM SHEPHERD\"            : \"german_shepherd\", # SO\n",
    "    \"GERMAN SHEPHERD\"          : \"german_shepherd\", # DA\n",
    "    \n",
    "    # ── AKITA ──\n",
    "    \"AKITA\"                    : \"akita\",        # SO, DA\n",
    "    \n",
    "    # ── HUSKY ──\n",
    "    \"ALASK MALAMUTE\"           : \"husky\",        # SO\n",
    "    \"ALASKAN HUSKY\"            : \"husky\",        # SO\n",
    "    \n",
    "    # ── CHIHUAHUA ──\n",
    "    \"CHIHUAHUA\"                : \"chihuahua\",    # DA\n",
    "    \n",
    "    # ── BOXER ──\n",
    "    \"BOXER\"                    : \"boxer\",        # DA\n",
    "    \n",
    "    # ── POODLE ──\n",
    "    \"POODLE\"                   : \"poodle\",       # DA\n",
    "    \n",
    "    # ── BEAGLE ──\n",
    "    \"BEAGLE\"                   : \"beagle\",       # DA\n",
    "    \n",
    "    # ── SHIH TZU ──\n",
    "    \"SHIH TZU\"                 : \"shih_tzu\",     # SO\n",
    "    \n",
    "    # ── TERRIER ──\n",
    "    \"AIREDALE TERR\"            : \"terrier\",      # SO\n",
    "    \"BULL TERRIER\"             : \"terrier\",      # DA\n",
    "    \"AFFENPINSCHER\"            : \"terrier\",      # SO\n",
    "    \n",
    "    # ── CAT DOMESTIC ──\n",
    "    \"DOMESTIC SH\"              : \"cat_short_hair\", # SO, DA\n",
    "    \"DOMESTIC LH\"              : \"cat_long_hair\", # DA\n",
    "    \"DOMESTIC MH\"              : \"cat_medium_hair\", # DA\n",
    "    \n",
    "    # ── UNKNOWN ──\n",
    "    \"UNKNOWN\"                  : \"unknown\",      # SJ\n",
    "}\n",
    "\n",
    "# Primary color mapping\n",
    "# The logic behind the grouping is to keep as much of the extra details regarding \n",
    "# patterns as possible while standardizing color groups.\n",
    "PRIMARY_COLOR_MAP = {\n",
    "    # ── BLACK VARIANTS ──\n",
    "    \"BLACK\"                    : \"black\",        # SO, SJ, DA\n",
    "    \"BLACK/WHITE\"              : \"black\",        # SO\n",
    "    \"BLACK/BLUE MERLE\"         : \"black_merle\",  # SO\n",
    "    \"BLACK/BRINDLE\"            : \"black_brindle\", # SO\n",
    "    \"BLACK/TABBY\"              : \"black_tabby\",  # SO\n",
    "    \"BLACK/TRICOLOR\"           : \"black_tricolor\", # SO\n",
    "\n",
    "    # ── WHITE ──\n",
    "    \"WHITE\"                    : \"white\",        # SO, SJ\n",
    "    \"WHITE/BLACK\"              : \"white\",        # SO\n",
    "    \"WHITE/GRAY\"               : \"white\",        # SO\n",
    "\n",
    "    # ── BROWN FAMILY ──\n",
    "    \"BROWN\"                    : \"brown\",        # SO\n",
    "    \"BROWN/WHITE\"              : \"brown\",        # SO\n",
    "    \"CHOCOLATE\"                : \"brown\",        # SO\n",
    "    \"CHOCOLATE/TABBY\"          : \"brown_tabby\",  # SO\n",
    "    \"BRINDLE/BROWN\"            : \"brown_brindle\", # SO\n",
    "\n",
    "    # ── GRAY / GREY ──\n",
    "    \"GRAY\"                     : \"gray\",         # SO\n",
    "    \"GREY\"                     : \"gray\",         # SJ\n",
    "    \"GRAY TABBY\"               : \"gray_tabby\",   # SO\n",
    "\n",
    "    # ── BLUE FAMILY ──\n",
    "    \"BLUE\"                     : \"blue\",         # SO, DA\n",
    "    \"BLUE MERLE\"               : \"blue_merle\",   # SO\n",
    "    \"BLUE CREAM\"               : \"blue\",         # SO\n",
    "    \"BLUE/WHITE\"               : \"blue\",         # SO\n",
    "\n",
    "    # ── ORANGE ──\n",
    "    \"ORANGE\"                   : \"orange\",       # SO\n",
    "    \"ORANGE/TABBY\"             : \"orange_tabby\", # SO\n",
    "\n",
    "    # ── CREAM / FAWN ──\n",
    "    \"CREAM\"                    : \"cream\",        # SO, SJ\n",
    "    \"FAWN\"                     : \"fawn\",         # DA\n",
    "    \"CREAM/TABBY\"              : \"cream_tabby\",  # SO\n",
    "\n",
    "    # ── CALICO / TORTIE ──\n",
    "    \"CALICO\"                   : \"calico\",       # SO\n",
    "    \"TORTIE\"                   : \"tortie\",       # SO\n",
    "    \"TORTIE/TABBY\"             : \"tortie_tabby\", # SO\n",
    "\n",
    "    # ── OTHER SPECIAL PATTERNS ──\n",
    "    \"TABBY/WHITE\"              : \"tabby\",        # SO\n",
    "    \"TRICOLOR\"                 : \"tricolor\",     # SO\n",
    "    \"SMOKE\"                    : \"smoke\",        # SO\n",
    "    \"TIGER/GRAY\"               : \"gray_tiger\",   # SO\n",
    "    \"POINT\"                    : \"point\",        # SO\n",
    "    \"TICK\"                     : \"tick\",         # SO\n",
    "\n",
    "    # ── RARE OR UNKNOWN ──\n",
    "    \"AGOUTI\"                   : \"other\",        # SO\n",
    "    \"AGOUTI/BRN TABBY\"         : \"other\",        # SO\n",
    "    \"0\"                        : \"other\",        # SJ\n",
    "}\n",
    "\n",
    "# Intake type mapping\n",
    "INTAKE_TYPE_MAP = {\n",
    "    # ── Born at facility ──\n",
    "    \"BORN HERE\"                : \"born_at_shelter\", # SO\n",
    "    \n",
    "    # ── Confiscated/Legal ──\n",
    "    \"CONFISCATE\"               : \"confiscated\",   # SJ, SO\n",
    "    \"CONFISCATED\"              : \"confiscated\",   # DA\n",
    "    \n",
    "    # ── Disposal/Euthanasia requests ──\n",
    "    \"DISPO REQ\"                : \"disposal_request\", # SJ\n",
    "    \"DISPOS REQ\"               : \"disposal_request\", # DA\n",
    "    \"EUTH REQ\"                 : \"euthanasia_request\", # SJ\n",
    "    \n",
    "    # ── Foster ──\n",
    "    \"FOSTER\"                   : \"foster\",        # DA, SJ\n",
    "    \n",
    "    # ── Protective custody/Quarantine ──\n",
    "    \"KEEPSAFE\"                 : \"protective_custody\", # DA\n",
    "    \"QUARANTINE\"               : \"protective_custody\", # SO\n",
    "    \n",
    "    # ── Resource/Treatment ──\n",
    "    \"RESOURCE\"                 : \"treatment\",     # DA\n",
    "    \"TREATMENT\"                : \"treatment\",     # DA\n",
    "    \n",
    "    # ── Return to owner ──\n",
    "    \"RETURN\"                   : \"return_to_owner\", # SJ\n",
    "    \n",
    "    # ── Spay/Neuter services ──\n",
    "    \"NEUTER\"                   : \"spay_neuter\",   # SJ\n",
    "    \"S/N CLINIC\"               : \"spay_neuter\",   # SJ\n",
    "    \"SPAY\"                     : \"spay_neuter\",   # SJ\n",
    "    \n",
    "    # ── Stray/TNR ──\n",
    "    \"STRAY\"                    : \"stray\",         # DA, SJ, SO\n",
    "    \"TNR\"                      : \"stray\",         # DA\n",
    "    \n",
    "    # ── Surrender/Returns ──\n",
    "    \"ADOPTION RETURN\"          : \"surrender\",     # SO\n",
    "    \"OS APPT\"                  : \"surrender\",     # SO\n",
    "    \"OWNER SUR\"                : \"surrender\",     # SJ\n",
    "    \"OWNER SURRENDER\"          : \"surrender\",     # DA, SO\n",
    "    \n",
    "    # ── Transfer ──\n",
    "    \"TRANSFER\"                 : \"transfer\",      # DA, SJ, SO\n",
    "    \n",
    "    # ── Wildlife ──\n",
    "    \"WILDLIFE\"                 : \"wildlife\",      # DA, SJ\n",
    "}\n",
    "\n",
    "# Intake condition mapping\n",
    "INTAKE_CONDITION_MAP = {\n",
    "    # ── Age-related ──\n",
    "    \"GERIATRIC\"                : \"age_related\",   # DA\n",
    "    \"UNDERAGE\"                 : \"age_related\",   # DA\n",
    "    \n",
    "    # ── Behavioral ──\n",
    "    \"AGGRESSIVE\"               : \"behavioral\",    # SJ\n",
    "    \"BEH M\"                    : \"behavioral\",    # SJ\n",
    "    \"BEH R\"                    : \"behavioral\",    # SJ\n",
    "    \"BEH U\"                    : \"behavioral\",    # SJ\n",
    "    \"FERAL\"                    : \"behavioral\",    # SJ\n",
    "    \n",
    "    # ── Critical/Severe ──\n",
    "    \"CRITICAL\"                 : \"critical\",      # DA\n",
    "    \"FATAL\"                    : \"critical\",      # DA\n",
    "    \"UNTREATABLE\"              : \"critical\",      # SC\n",
    "    \n",
    "    # ── Deceased ──\n",
    "    \"DECEASED\"                 : \"deceased\",      # DA\n",
    "    \"DEAD\"                     : \"deceased\",      # SJ\n",
    "    \n",
    "    # ── Healthy/Normal ──\n",
    "    \"APP WNL\"                  : \"healthy\",       # DA\n",
    "    \"NORMAL\"                   : \"healthy\",       # DA\n",
    "    \"HEALTHY\"                  : \"healthy\",       # SJ, SC\n",
    "    \n",
    "    # ── Medical ──\n",
    "    \"APP INJ\"                  : \"medical\",       # DA\n",
    "    \"APP SICK\"                 : \"medical\",       # DA\n",
    "    \"MED EMERG\"                : \"medical\",       # SJ\n",
    "    \"MED M\"                    : \"medical\",       # SJ\n",
    "    \"MED R\"                    : \"medical\",       # SJ\n",
    "    \"MED SEV\"                  : \"medical\",       # SJ\n",
    "    \"TREATABLE/MANAGEABLE\"     : \"medical\",       # SC\n",
    "    \"TREATABLE/REHAB\"          : \"medical\",       # SC\n",
    "    \n",
    "    # ── Reproductive ──\n",
    "    \"NURSING\"                  : \"reproductive\",  # SJ\n",
    "    \"PREGNANT\"                 : \"reproductive\",  # SJ\n",
    "    \n",
    "    # ── Unknown/Other ──\n",
    "    \"UNKNOWN\"                  : \"unknown\",       # SC\n",
    "}\n",
    "\n",
    "# Intake reason mapping\n",
    "INTAKE_REASON_MAP = {\n",
    "    # ── Adoption related ──\n",
    "    \"FOR ADOPT\"                : \"for_adoption\",  # DA\n",
    "    \"FOR PLCMNT\"               : \"for_adoption\",  # DA\n",
    "    \"IP ADOPT\"                 : \"for_adoption\",  # SJ\n",
    "    \n",
    "    # ── Behavioral issues ──\n",
    "    \"BEHAVIOR\"                 : \"behavior\",      # DA\n",
    "    \"AGG ANIMAL\"               : \"behavior\",      # SJ\n",
    "    \"AGG PEOPLE\"               : \"behavior\",      # SJ\n",
    "    \"BITES\"                    : \"behavior\",      # SJ\n",
    "    \"CHASES ANI\"               : \"behavior\",      # SJ\n",
    "    \"DESTRUC IN\"               : \"behavior\",      # SJ\n",
    "    \"ESCAPES\"                  : \"behavior\",      # SJ\n",
    "    \"HOUSE SOIL\"               : \"behavior\",      # SJ\n",
    "    \"HYPER\"                    : \"behavior\",      # SJ\n",
    "    \"NOFRIENDLY\"               : \"behavior\",      # SJ\n",
    "    \"PICA\"                     : \"behavior\",      # SJ\n",
    "    \n",
    "    # ── Breeding restrictions ──\n",
    "    \"BREED REST\"               : \"breed_restriction\", # DA\n",
    "    \n",
    "    # ── Euthanasia ──\n",
    "    \"OWR REQ EU\"               : \"owner_requested_euthanasia\", # DA\n",
    "    \"IP EUTH\"                  : \"owner_requested_euthanasia\", # SJ\n",
    "    \n",
    "    # ── Medical ──\n",
    "    \"MEDICAL\"                  : \"medical\",       # DA\n",
    "    \"SURGERY\"                  : \"medical\",       # DA\n",
    "    \"VET CARE\"                 : \"medical\",       # DA\n",
    "    \n",
    "    # ── Other ──\n",
    "    \"OTHER\"                    : \"other\",         # DA\n",
    "    \"OTHRINTAKS\"               : \"other\",         # DA\n",
    "    \n",
    "    # ── Owner surrender for various reasons ──\n",
    "    \"CANTAFFORD\"               : \"owner_surrender\", # DA\n",
    "    \"EVICTION\"                 : \"owner_surrender\", # DA\n",
    "    \"FINANCIAL\"                : \"owner_surrender\", # DA\n",
    "    \"HOUSING\"                  : \"owner_surrender\", # DA\n",
    "    \"LLCONFLICT\"               : \"owner_surrender\", # DA\n",
    "    \"LOSSHOUSNG\"               : \"owner_surrender\", # DA\n",
    "    \"PETDEPFEE\"                : \"owner_surrender\", # DA\n",
    "    \"LANDLORD\"                 : \"owner_surrender\", # SJ\n",
    "    \"MOVE\"                     : \"owner_surrender\", # SJ\n",
    "    \"NO HOME\"                  : \"owner_surrender\", # SJ\n",
    "    \"OWR DEATH\"                : \"owner_surrender\", # DA\n",
    "    \"PERLIFECNG\"               : \"owner_surrender\", # DA\n",
    "    \"PERSNLISSU\"               : \"owner_surrender\", # DA\n",
    "    \"TEMLIFECNG\"               : \"owner_surrender\", # DA\n",
    "    \"ALLERGIC\"                 : \"owner_surrender\", # SJ\n",
    "    \"CHILD PROB\"               : \"owner_surrender\", # SJ\n",
    "    \"NO TIME\"                  : \"owner_surrender\", # SJ\n",
    "    \"OWNER DIED\"               : \"owner_surrender\", # SJ\n",
    "    \"OWNER PROB\"               : \"owner_surrender\", # SJ\n",
    "    \"TRAVEL\"                   : \"owner_surrender\", # SJ\n",
    "    \"NOTRIGHTFT\"               : \"owner_surrender\", # DA\n",
    "    \"ATTENTION\"                : \"owner_surrender\", # SJ\n",
    "    \"OTHER PET\"                : \"owner_surrender\", # SJ\n",
    "    \"TOO BIG\"                  : \"owner_surrender\", # SJ\n",
    "    \"TOO MANY\"                 : \"owner_surrender\", # SJ\n",
    "    \n",
    "    # ── Stray/Found ──\n",
    "    \"STRAY\"                    : \"stray\",         # DA\n",
    "    \n",
    "    # ── Temporary/Short-term ──\n",
    "    \"SHORT-TERM\"               : \"temporary_care\", # DA\n",
    "    \n",
    "    # ── TNR/Clinic ──\n",
    "    \"TNR CLINIC\"               : \"trap_neuter_return\", # DA\n",
    "    \n",
    "    # ── Transfers ──\n",
    "    \"TRANSFER\"                 : \"transfer\",      # DA\n",
    "}\n",
    "\n",
    "# Outcome type mapping\n",
    "OUTCOME_TYPE_MAP = {\n",
    "    # ── Adoption & Rescue ──\n",
    "    \"ADOPTION\"                 : \"adoption\",      # DA, SJ, SO\n",
    "    \"RESCUE\"                   : \"adoption\",      # SJ\n",
    "    \n",
    "    # ── Death/Euthanasia ──\n",
    "    \"DIED\"                     : \"deceased\",      # DA, SJ, SO\n",
    "    \"EUTH\"                     : \"euthanasia\",    # SJ\n",
    "    \"EUTHANIZE\"                : \"euthanasia\",    # SO\n",
    "    \"EUTHANIZED\"               : \"euthanasia\",    # DA\n",
    "    \"REQ EUTH\"                 : \"euthanasia\",    # SJ\n",
    "    \n",
    "    # ── Disposal/Other deaths ──\n",
    "    \"DISPOSAL\"                 : \"disposal\",      # DA, SJ, SO\n",
    "    \n",
    "    # ── Escaped/Missing/Lost ──\n",
    "    \"ESCAPED/STOLEN\"           : \"escaped\",       # SO\n",
    "    \"FOUND ANIM\"               : \"found\",         # SJ\n",
    "    \"FOUND EXP\"                : \"found\",         # DA\n",
    "    \"LOST EXP\"                 : \"lost\",          # DA, SJ\n",
    "    \"MISSING\"                  : \"lost\",          # DA, SJ\n",
    "    \n",
    "    # ── Foster ──\n",
    "    \"FOSTER\"                   : \"foster\",        # DA, SJ\n",
    "    \n",
    "    # ── Medical/Treatment ──\n",
    "    \"TREATMENT\"                : \"treatment\",     # DA\n",
    "    \"VET\"                      : \"treatment\",     # SO\n",
    "    \n",
    "    # ── Other/Closed/Unknown ──\n",
    "    \"CLOSED\"                   : \"other\",         # DA\n",
    "    \"OTHER\"                    : \"other\",         # DA\n",
    "    \n",
    "    # ── Return to Owner ──\n",
    "    \"RETURN TO OWNER\"          : \"return_to_owner\", # SO\n",
    "    \"RETURNED TO OWNER\"        : \"return_to_owner\", # DA\n",
    "    \"RTF\"                      : \"return_to_field\", # SJ\n",
    "    \"RTO\"                      : \"return_to_owner\", # SJ\n",
    "    \"RTOS\"                     : \"return_to_owner\", # SO\n",
    "    \n",
    "    # ── Spay/Neuter Services ──\n",
    "    \"NEUTER\"                   : \"spay_neuter\",   # SJ\n",
    "    \"SNR\"                      : \"spay_neuter\",   # DA\n",
    "    \"SPAY\"                     : \"spay_neuter\",   # SJ\n",
    "    \n",
    "    # ── TNR/Release ──\n",
    "    \"TNR\"                      : \"trap_neuter_release\", # DA\n",
    "    \n",
    "    # ── Transfer ──\n",
    "    \"TRANSFER\"                 : \"transfer\",      # DA, SJ, SO\n",
    "    \n",
    "    # ── Wildlife ──\n",
    "    \"WILDLIFE\"                 : \"wildlife\",      # DA\n",
    "}\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────\n",
    "# FINAL ASSEMBLY\n",
    "# ──────────────────────────────────────────────────────────────────\n",
    "VALUE_MAPPINGS = {\n",
    "    \"animal_type\"      : ANIMAL_TYPE_MAP,\n",
    "    \"breed\"            : BREED_MAP,\n",
    "    \"primary_color\"    : PRIMARY_COLOR_MAP,\n",
    "    \"intake_type\"      : INTAKE_TYPE_MAP,\n",
    "    \"intake_condition\" : INTAKE_CONDITION_MAP,\n",
    "    \"intake_reason\"    : INTAKE_REASON_MAP,\n",
    "    \"outcome_type\"     : OUTCOME_TYPE_MAP,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Inconsistency Challenge:** Raw datasets contain identical concepts with slight variations (\"CONFISCATED\", \"CONFISCATE\", \"CONFISCTED\"), creating data fragmentation that would break downstream analysis.\n",
    "\n",
    "The `apply_silver_transforms` function solves this by:\n",
    "\n",
    "- **Harmonizing categorical values** using VALUE_MAPPINGS to create consistent vocabulary\n",
    "- **Enforcing uniform data types** across all sources for optimal performance  \n",
    "- **Validating dates** and flagging future entries as data quality issues\n",
    "- **Handling missing columns** gracefully across different data sources\n",
    "\n",
    "**Result:** All datasets now share a common structure and vocabulary for reliable analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Execute Transformations\n",
    "\n",
    "**Purpose:**  \n",
    "Apply all silver-layer transformations to each dataset in one streamlined operation.\n",
    "\n",
    "**What happens:** Each raw dataset gets standardized values, proper data types, and quality validation applied automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Found 1 future dates in intake_date for dallas\n",
      "         Latest future date: 2025-09-27\n",
      "         Setting future dates to NaT (Not a Time)\n",
      "Silver transformations applied successfully!\n"
     ]
    }
   ],
   "source": [
    "# Apply transformations\n",
    "SILVER_DFS = {\n",
    "    source: apply_silver_transforms(df, source)\n",
    "    for source, df in CLEAN_DFS.items()\n",
    "}\n",
    "\n",
    "print(\"Silver transformations applied successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After Value Harmonization\n",
    "\n",
    "Compare the dramatic reduction in unique values after applying our standardization mappings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AFTER VALUE HARMONIZATION:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_83eba\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_83eba_level0_col0\" class=\"col_heading level0 col0\" >animal_type</th>\n",
       "      <th id=\"T_83eba_level0_col1\" class=\"col_heading level0 col1\" >breed</th>\n",
       "      <th id=\"T_83eba_level0_col2\" class=\"col_heading level0 col2\" >intake_type</th>\n",
       "      <th id=\"T_83eba_level0_col3\" class=\"col_heading level0 col3\" >intake_condition</th>\n",
       "      <th id=\"T_83eba_level0_col4\" class=\"col_heading level0 col4\" >intake_reason</th>\n",
       "      <th id=\"T_83eba_level0_col5\" class=\"col_heading level0 col5\" >outcome_type</th>\n",
       "      <th id=\"T_83eba_level0_col6\" class=\"col_heading level0 col6\" >primary_color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_83eba_level0_row0\" class=\"row_heading level0 row0\" >dallas</th>\n",
       "      <td id=\"T_83eba_row0_col0\" class=\"data row0 col0\" >3</td>\n",
       "      <td id=\"T_83eba_row0_col1\" class=\"data row0 col1\" >13</td>\n",
       "      <td id=\"T_83eba_row0_col2\" class=\"data row0 col2\" >9</td>\n",
       "      <td id=\"T_83eba_row0_col3\" class=\"data row0 col3\" >5</td>\n",
       "      <td id=\"T_83eba_row0_col4\" class=\"data row0 col4\" >12</td>\n",
       "      <td id=\"T_83eba_row0_col5\" class=\"data row0 col5\" >14</td>\n",
       "      <td id=\"T_83eba_row0_col6\" class=\"data row0 col6\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_83eba_level0_row1\" class=\"row_heading level0 row1\" >san_jose</th>\n",
       "      <td id=\"T_83eba_row1_col0\" class=\"data row1 col0\" >3</td>\n",
       "      <td id=\"T_83eba_row1_col1\" class=\"data row1 col1\" >13</td>\n",
       "      <td id=\"T_83eba_row1_col2\" class=\"data row1 col2\" >10</td>\n",
       "      <td id=\"T_83eba_row1_col3\" class=\"data row1 col3\" >5</td>\n",
       "      <td id=\"T_83eba_row1_col4\" class=\"data row1 col4\" >5</td>\n",
       "      <td id=\"T_83eba_row1_col5\" class=\"data row1 col5\" >12</td>\n",
       "      <td id=\"T_83eba_row1_col6\" class=\"data row1 col6\" >11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_83eba_level0_row2\" class=\"row_heading level0 row2\" >soco</th>\n",
       "      <td id=\"T_83eba_row2_col0\" class=\"data row2 col0\" >3</td>\n",
       "      <td id=\"T_83eba_row2_col1\" class=\"data row2 col1\" >14</td>\n",
       "      <td id=\"T_83eba_row2_col2\" class=\"data row2 col2\" >6</td>\n",
       "      <td id=\"T_83eba_row2_col3\" class=\"data row2 col3\" >4</td>\n",
       "      <td id=\"T_83eba_row2_col4\" class=\"data row2 col4\" >1</td>\n",
       "      <td id=\"T_83eba_row2_col5\" class=\"data row2 col5\" >9</td>\n",
       "      <td id=\"T_83eba_row2_col6\" class=\"data row2 col6\" >16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1733c3250>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"AFTER VALUE HARMONIZATION:\")\n",
    "create_cardinality_summary(SILVER_DFS, categorical_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Findings:`\n",
    "- **Consistent animal_type**: All sources now have exactly 3 categories (dog, cat, other)\n",
    "- **Manageable breed categories**: Reduced to ~13-14 standardized groups per source  \n",
    "- **Data gaps identified**: Dallas missing primary_color data (0 values), Soco has minimal intake_reason data (1 value)\n",
    "\n",
    "The harmonization successfully created a unified vocabulary while revealing data quality patterns across sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Silver\n",
    "\n",
    "**Purpose:**  \n",
    "Combine each source’s cleaned DataFrame into the final `silver_df` according to our `FINAL_SCHEMA`, and if desired, do a data quality assesment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_SCHEMA = [\n",
    "    \"animal_id\", \"animal_type\", \"breed\", \"primary_color\", \"age\",  \"sex\",\n",
    "    \"intake_type\", \"intake_condition\", \"intake_reason\", \"intake_date\",\n",
    "    \"outcome_type\", \"outcome_date\", \"region\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silver dataset created: 111,907 records × 13 columns\n",
      "Duplicates removed: 0\n"
     ]
    }
   ],
   "source": [
    "# Here we create the final silver dataset\n",
    "silver_df = create_silver_dataset(SILVER_DFS, FINAL_SCHEMA)\n",
    "print(f\"Silver dataset created: {silver_df.shape[0]:,} records × {silver_df.shape[1]} columns\")\n",
    "print(f\"Duplicates removed: {sum(df.shape[0] for df in SILVER_DFS.values()) - silver_df.shape[0]:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Post-Processing of the Silver Data\n",
    "\n",
    "After joining all the data together in a single data-frame, we are using imputation to further refine our dataset across different features.\n",
    "\n",
    "> Note to self: Group one liners where necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1 Compute Age from Intake and DOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "silver_df = compute_age_from_dates(silver_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2 Apply species-specific median imputations for missing ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputed 60488 missing ages for species: dog (median=2.00)\n",
      "Imputed 33149 missing ages for species: cat (median=1.00)\n",
      "Imputed 8966 missing ages for species: other (median=1.50)\n"
     ]
    }
   ],
   "source": [
    "silver_df = impute_missing_age(silver_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.3 Bin Ages into 3 Life Stages (Puppy/Kitten, Adult, Senior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 'age_stage' column with categories: puppy/kitten, adult, senior\n"
     ]
    }
   ],
   "source": [
    "silver_df = bin_age_into_life_stages(silver_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4 Extract Sex from Health, Apply Imputation for Missing Sex\n",
    "\n",
    "> Note: Missspelled function name need to fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "silver_df = recatogarize_sex(silver_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.5 Apply Imputation to Assign Sex Based on Group Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "silver_df = impute_sex_by_species_and_breed(silver_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.6 Apply Imputation to Assign Primary Color for Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "silver_df = impute_primary_color_by_species_and_breed(silver_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Materialize Silver\n",
    "\n",
    "**Purpose:**  \n",
    "Materialize the final Silver data, with all the cleaning done above.\n",
    "\n",
    "This allows us to build Gold on top of this cleaned data, without having to re-process the data every time. This makes the work more efficient, and follows best practices. \n",
    "\n",
    "All ad-hoc analytics, data exploration, analysis, etc. should be done on top of this data, as it is validated and consistent. This sets a strong foundation for the rest of the data work for our team.\n",
    "\n",
    "Since we do not have a Database, as is common when using Medallion architecture, we are materializing the data by writing it to `.parquet`. Parquet allows for faster analysis, preserves data types for data, and is an efficient standard for data-storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved silver df to ../../data-assets/silver/silver.parquet\n"
     ]
    }
   ],
   "source": [
    "SILVER_DF_PATH = \"../../data-assets/silver/silver.parquet\"\n",
    "os.makedirs(os.path.dirname(SILVER_DF_PATH), exist_ok=True)\n",
    "\n",
    "silver_df.to_parquet(SILVER_DF_PATH, index=False)\n",
    "print(f\"Saved silver df to {SILVER_DF_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Data Quality Assessment\n",
    "\n",
    "Comprehensive quality checks and data profiling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATA QUALITY PROFILE\n",
      "============================================================\n",
      "\n",
      "DATASET OVERVIEW\n",
      "Total records: 111,907\n",
      "Total columns: 14\n",
      "\n",
      "MISSING DATA ANALYSIS\n",
      "  intake_date: 1 (0.001%)\n",
      "  outcome_date: 2,134 (1.907%)\n",
      "\n",
      "CARDINALITY ANALYSIS\n",
      "  animal_id: 91,523 unique values\n",
      "  intake_date: 3,984 unique values\n",
      "  outcome_date: 3,599 unique values\n",
      "  age: 29 unique values\n",
      "  primary_color: 16 unique values\n",
      "  outcome_type: 16 unique values\n",
      "  breed: 14 unique values\n",
      "  intake_type: 13 unique values\n",
      "  intake_reason: 12 unique values\n",
      "  intake_condition: 8 unique values\n",
      "  age_stage: 4 unique values\n",
      "  animal_type: 3 unique values\n",
      "  region: 3 unique values\n",
      "  sex: 2 unique values\n",
      "\n",
      "INTAKE_TYPE DISTRIBUTION\n",
      "  stray: 59.3%\n",
      "  surrender: 12.1%\n",
      "  foster: 11.4%\n",
      "  confiscated: 6.5%\n",
      "  treatment: 3.0%\n",
      "  disposal_request: 2.2%\n",
      "  wildlife: 1.8%\n",
      "  protective_custody: 1.7%\n",
      "  spay_neuter: 0.9%\n",
      "  transfer: 0.7%\n",
      "\n",
      "ANIMAL_TYPE DISTRIBUTION\n",
      "  dog: 56.9%\n",
      "  cat: 35.0%\n",
      "  other: 8.1%\n",
      "\n",
      "BREED DISTRIBUTION\n",
      "  other: 53.5%\n",
      "  cat_short_hair: 29.6%\n",
      "  pit_bull: 5.3%\n",
      "  german_shepherd: 3.0%\n",
      "  cat_medium_hair: 2.8%\n",
      "  labrador: 1.5%\n",
      "  mixed: 1.4%\n",
      "  cat_long_hair: 1.3%\n",
      "  husky: 0.7%\n",
      "  shih_tzu: 0.4%\n",
      "\n",
      "PRIMARY_COLOR DISTRIBUTION\n",
      "  other: 45.6%\n",
      "  black: 22.1%\n",
      "  white: 10.5%\n",
      "  brown: 9.0%\n",
      "  gray: 4.8%\n",
      "  tricolor: 2.5%\n",
      "  blue: 0.9%\n",
      "  gray_tabby: 0.9%\n",
      "  cream: 0.8%\n",
      "  orange: 0.7%\n",
      "\n",
      "INTAKE_CONDITION DISTRIBUTION\n",
      "  healthy: 65.6%\n",
      "  medical: 13.6%\n",
      "  unknown: 10.1%\n",
      "  critical: 3.1%\n",
      "  deceased: 2.8%\n",
      "  age_related: 1.8%\n",
      "  reproductive: 1.6%\n",
      "  behavioral: 1.5%\n",
      "\n",
      "INTAKE_REASON DISTRIBUTION\n",
      "  unknown: 45.8%\n",
      "  other: 29.2%\n",
      "  owner_surrender: 7.0%\n",
      "  medical: 6.1%\n",
      "  for_adoption: 5.7%\n",
      "  stray: 2.5%\n",
      "  transfer: 1.6%\n",
      "  behavior: 1.2%\n",
      "  trap_neuter_return: 0.4%\n",
      "  temporary_care: 0.4%\n",
      "\n",
      "OUTCOME_TYPE DISTRIBUTION\n",
      "  adoption: 28.7%\n",
      "  return_to_owner: 14.9%\n",
      "  transfer: 14.4%\n",
      "  euthanasia: 12.6%\n",
      "  foster: 11.1%\n",
      "  lost: 3.1%\n",
      "  disposal: 2.8%\n",
      "  other: 2.8%\n",
      "  return_to_field: 2.2%\n",
      "  treatment: 2.0%\n",
      "\n",
      "ANIMAL_TYPE DISTRIBUTION\n",
      "  dog: 56.9%\n",
      "  cat: 35.0%\n",
      "  other: 8.1%\n",
      "\n",
      "TEMPORAL ANALYSIS\n",
      "  Intake date range: 2013-08-16 to 2025-05-24\n",
      "  Average monthly intake: 788 animals\n",
      "  Peak month: July 2024 (6,079 animals)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bc/lrkcgdlx3332x9brp_np1l_c0000gn/T/ipykernel_37340/2220793067.py:205: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  monthly = df.set_index('intake_date').resample('M').size()\n"
     ]
    }
   ],
   "source": [
    "# Lets generate the data profile for the silver dataset\n",
    "generate_data_overview(silver_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. References  \n",
    "Databricks. (n.d.). *Medallion Architecture*. Retrieved May 10, 2025, from https://www.databricks.com/glossary/medallion-architecture"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
