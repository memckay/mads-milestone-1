{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Bronze Data Load**\n",
    "\n",
    "**Purpose:**  Ingest raw data from all sources into the Bronze layer with **no business logic** or feature engineering—only the bare minimum of cleaning required for schema alignment.\n",
    "\n",
    "**What this notebook does:**  \n",
    "1. **Reads** data from:  \n",
    "   - San Jose API (JSON → DataFrame)  \n",
    "   - Dallas CSV  \n",
    "   - SoCo CSV  \n",
    "2. **Materializes** the data into our \"tables\":\n",
    "   - `data-assets/bronze/dallas_df.parquet`\n",
    "   - `data-assets/bronze/san_jose_df.parquet`\n",
    "   - `data-assets/bronze/soco_df.parquet`\n",
    "\n",
    "This data will be used when creating [Silver](./2_silver.ipynb), where it will be cleaned and pre-processed to allow us to work with higher quality data.\n",
    "\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    A([Bronze])\n",
    "    %% Bronze styling - brown/bronze color\n",
    "    style A fill:#cd7f32,stroke:#8b4513,stroke-width:3px,color:#fff\n",
    "    B([Silver])\n",
    "    C([Gold])\n",
    "\n",
    "    A --> B\n",
    "    B --> C\n",
    "```\n",
    "\n",
    "<div>\n",
    "\n",
    "For more on Medallion Architecture, see [Databricks Glossary: Medallion Architecture](https://www.databricks.com/glossary/medallion-architecture) (Databricks, n.d.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Table of Contents**\n",
    "\n",
    "1. [Setup](#1-setup)  \n",
    "   Install dependencies and import essential libraries for data processing\n",
    "\n",
    "1. [Configuration](#2-configuration)  \n",
    "   Define paths, API endpoints\n",
    "\n",
    "1. [Data Loading](#3-data-loading)   \n",
    "   Fetch regional shelter data from APIs and CSV files, save to bronze layer\n",
    "\n",
    "1. [Materialize Bronze](#4-materialize-bronze)   \n",
    "   Process raw data into bronze parquet files to use in Silver for cleaning\n",
    "\n",
    "1. [References](#5-references)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Setup**\n",
    "\n",
    "**Purpose:**  Ensure the environment has all necessary libraries installed and imported.  \n",
    "```python\n",
    "# Install project-wide dependencies\n",
    "%pip install -r ../../requirements.txt\n",
    "``` \n",
    "\n",
    "> **Note:** we use a project-wide `requirements.txt` for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'base (Python 3.11.8)' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "%pip install -r ../../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Configuration**\n",
    "\n",
    "**Purpose:**  \n",
    "Centralize all “magic” values—file paths, API endpoints, parameters, and date-column names to make it easy to load everything locally.\n",
    "- Makes the notebook reproducible.  \n",
    "- Keeps the loading cells concise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Configuration ───\n",
    "\n",
    "# File paths for the CSV files\n",
    "DATA_DIR = \"../../data-assets/_raw\"\n",
    "CSV_PATHS = {\n",
    "    \"dallas\": os.path.join(\n",
    "        DATA_DIR, \"Dallas_Animal_Shelter_Data_Fiscal_Year_2023_-_2025_20250516.csv\"\n",
    "    ),\n",
    "    \"soco\": os.path.join(\n",
    "        DATA_DIR, \"SoCo_Animal_Shelter_Intake_and_Outcome_20250519.csv\"\n",
    "    ),\n",
    "}\n",
    "API_PATHS = {\n",
    "    \"san_jose\": {\n",
    "        \"base_url\": \"https://data.sanjoseca.gov/api/3/action/datastore_search\",\n",
    "        # The default limit for the API is 100, so we had to add a limit to get all the data\n",
    "        # The expected amount of rows is 16,274 as of 2025-05-25\n",
    "        \"params\": {\n",
    "            \"resource_id\": \"f3354a37-7e03-41f8-a94d-3f720389a68a\",\n",
    "            \"limit\": 1000000,\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "# ─── Date columns to parse ───\n",
    "DATE_COLS = {\n",
    "    \"san_jose\": [\"IntakeDate\", \"OutcomeDate\"],\n",
    "    \"dallas\": [\"Intake_Date\", \"Outcome_Date\"],\n",
    "    \"soco\": [\"Intake Date\", \"Outcome Date\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Data Loading**\n",
    "\n",
    "**Purpose:**  \n",
    "Pull raw data into pandas DataFrames:  \n",
    "-  Call the San Jose API and parse its JSON response.  \n",
    "-  Read the Dallas + SoCo CSV files, converting date strings to `datetime64`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Data Loading ───\n",
    "\n",
    "# San Jose API\n",
    "resp = requests.get(\n",
    "    API_PATHS[\"san_jose\"][\"base_url\"], params=API_PATHS[\"san_jose\"][\"params\"]\n",
    ")\n",
    "resp.raise_for_status()\n",
    "san_jose_df = pd.DataFrame(resp.json()[\"result\"][\"records\"])\n",
    "\n",
    "# Parse as datetimes\n",
    "for col in DATE_COLS[\"san_jose\"]:\n",
    "    san_jose_df[col] = pd.to_datetime(san_jose_df[col], errors=\"coerce\")\n",
    "\n",
    "\n",
    "def read(name: str):\n",
    "    return pd.read_csv(CSV_PATHS[name], parse_dates=DATE_COLS[name], low_memory=False)\n",
    "\n",
    "\n",
    "# Dallas and SoCo CSVs\n",
    "dallas_df = read(\"dallas\")\n",
    "soco_df = read(\"soco\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Materialize Bronze**\n",
    "\n",
    "**Purpose:**  \n",
    "Materialize the Bronze data, as is from the source.\n",
    "\n",
    "This allows us to have a copy of the data for reproducability, and if we need to re-build Silver. By materializing this data, we avoid re-incurring the costs of pulling down data from an API/download a csv, and store it as-is for future use-cases, in a parquet format.\n",
    "\n",
    "Since we do not have a Database, as is common when using Medallion architecture, we are materializing the data by writing it to `.parquet`. Parquet allows for faster analysis, preserves data types for data, and is an efficient standard for data-storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved san_jose_df to ../../data-assets/bronze/san_jose_df.parquet\n",
      "Saved dallas_df to ../../data-assets/bronze/dallas_df.parquet\n",
      "Saved soco_df to ../../data-assets/bronze/soco_df.parquet\n"
     ]
    }
   ],
   "source": [
    "BRONZE_DIR = \"../../data-assets/bronze\"\n",
    "os.makedirs(BRONZE_DIR, exist_ok=True)\n",
    "\n",
    "dfs = [san_jose_df, dallas_df, soco_df]\n",
    "\n",
    "for df in dfs:\n",
    "    df_name = [name for name, obj in globals().items() if obj is df][0]\n",
    "    df.to_parquet(os.path.join(BRONZE_DIR, f\"{df_name}.parquet\"), index=False)\n",
    "    print(f\"Saved {df_name} to {BRONZE_DIR}/{df_name}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "san_jose_df shape: (16490, 22)\n",
      "dallas_df shape: (65079, 34)\n",
      "soco_df shape: (30554, 24)\n"
     ]
    }
   ],
   "source": [
    "for df in dfs:\n",
    "    df_name = [name for name, obj in globals().items() if obj is df][0]\n",
    "    print(f\"{df_name} shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> -> Click to go to [Silver Layer](./2_silver.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. References**  \n",
    "Databricks. (n.d.). *Medallion Architecture*. Retrieved May 10, 2025, from https://www.databricks.com/glossary/medallion-architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
