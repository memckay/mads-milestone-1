{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bronze Data Load\n",
    "\n",
    "**Purpose:**  \n",
    "Ingest raw data from all sources into the Bronze layer with **no business logic** or feature engineering—only the bare minimum of cleaning required for schema alignment.\n",
    "\n",
    "**What this notebook does:**  \n",
    "1. **Reads** data from:  \n",
    "   - San Jose API (JSON → DataFrame)  \n",
    "   - Dallas CSV  \n",
    "   - SoCo CSV  \n",
    "2. **Renames** columns to a common, lowercase, snake_case style  \n",
    "3. **Tags** each row with its `source` (provenance)  \n",
    "4. **Deduplicates** any exact‐duplicate records  \n",
    "5. **Preserves** every original field (e.g. SoCo’s `Date of Birth`, Dallas’s `Kennel_Number`) so downstream layers can decide what to keep\n",
    "\n",
    "For more on Medallion Architecture, see [Databricks Glossary: Medallion Architecture](https://www.databricks.com/glossary/medallion-architecture) (Databricks, n.d.).\n",
    "\n",
    "---\n",
    "\n",
    "### References  \n",
    "Databricks. (n.d.). *Medallion Architecture*. Retrieved May 10, 2025, from https://www.databricks.com/glossary/medallion-architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. [Setup](#setup)  \n",
    "   Install required packages and import libraries.  \n",
    "\n",
    "2. [Configuration](#configuration)  \n",
    "   Define all file paths, API parameters, and date-column lists in one place for reproducibility.  \n",
    "\n",
    "3. [Data Loading](#data-loading)  \n",
    "   Fetch data from the San Jose API and read the Dallas + SoCo CSVs into pandas DataFrames.  \n",
    "\n",
    "4. [Data Cleaning & Standardization](#data-cleaning--standardization)  \n",
    "   Rename columns, lowercase them, and drop duplicates so all sources share the same schema.  \n",
    "\n",
    "5. [Data Merging & Harmonization](#data-merging--harmonization)  \n",
    "   Stack the three sources into one “bronze” table, tag each row by origin, and enforce dtypes.  \n",
    "\n",
    "6. [Quick Exploratory Checks](#quick-exploratory-checks)  \n",
    "   Check missing values, unique counts, distributions, date ranges, and monthly trends.  \n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "**Purpose:**  \n",
    "Ensure the environment has all necessary libraries installed and imported.  \n",
    "- `%pip install ...` installs dependencies.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kagglehub==0.3.12 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages (from -r ../../requirements.txt (line 1)) (0.3.12)\n",
      "Requirement already satisfied: seaborn==0.13.2 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages (from -r ../../requirements.txt (line 2)) (0.13.2)\n",
      "Requirement already satisfied: pandas==2.2.2 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages (from -r ../../requirements.txt (line 3)) (2.2.2)\n",
      "Requirement already satisfied: matplotlib==3.9.2 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages (from -r ../../requirements.txt (line 4)) (3.9.2)\n",
      "Requirement already satisfied: sodapy==2.2.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages (from -r ../../requirements.txt (line 5)) (2.2.0)\n",
      "Requirement already satisfied: packaging in /opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages (from kagglehub==0.3.12->-r ../../requirements.txt (line 1)) (24.0)\n",
      "Requirement already satisfied: pyyaml in /opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages (from kagglehub==0.3.12->-r ../../requirements.txt (line 1)) (6.0.1)\n",
      "Requirement already satisfied: requests in /opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages (from kagglehub==0.3.12->-r ../../requirements.txt (line 1)) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages (from kagglehub==0.3.12->-r ../../requirements.txt (line 1)) (4.65.0)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages (from seaborn==0.13.2->-r ../../requirements.txt (line 2)) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages (from pandas==2.2.2->-r ../../requirements.txt (line 3)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages (from pandas==2.2.2->-r ../../requirements.txt (line 3)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages (from pandas==2.2.2->-r ../../requirements.txt (line 3)) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages (from matplotlib==3.9.2->-r ../../requirements.txt (line 4)) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages (from matplotlib==3.9.2->-r ../../requirements.txt (line 4)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages (from matplotlib==3.9.2->-r ../../requirements.txt (line 4)) (4.58.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages (from matplotlib==3.9.2->-r ../../requirements.txt (line 4)) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages (from matplotlib==3.9.2->-r ../../requirements.txt (line 4)) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages (from matplotlib==3.9.2->-r ../../requirements.txt (line 4)) (3.2.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas==2.2.2->-r ../../requirements.txt (line 3)) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages (from requests->kagglehub==0.3.12->-r ../../requirements.txt (line 1)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages (from requests->kagglehub==0.3.12->-r ../../requirements.txt (line 1)) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages (from requests->kagglehub==0.3.12->-r ../../requirements.txt (line 1)) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages (from requests->kagglehub==0.3.12->-r ../../requirements.txt (line 1)) (2024.6.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r ../../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from sodapy import Socrata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## 2. Configuration\n",
    "\n",
    "**Purpose:**  \n",
    "Centralize all “magic” values—file paths, API endpoints, parameters, and date-column names to make it easy to load everything locally.\n",
    "- Makes the notebook reproducible.  \n",
    "- Keeps the loading cells concise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Configuration ───\n",
    "\n",
    "# File paths for the CSV files\n",
    "DATA_DIR = \"../../data-assets/bronze\"\n",
    "CSV_PATHS = {\n",
    "    \"dallas\": os.path.join(\n",
    "        DATA_DIR,\n",
    "        \"Dallas_Animal_Shelter_Data_Fiscal_Year_2023_-_2025_20250516.csv\"\n",
    "    ),\n",
    "    \"soco\": os.path.join(\n",
    "        DATA_DIR,\n",
    "        \"SoCo_Animal_Shelter_Intake_and_Outcome_20250519.csv\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "SAN_JOSE = {\n",
    "    \"base_url\":    \"https://data.sanjoseca.gov/api/3/action/datastore_search\",\n",
    "    \"resource_id\": \"f3354a37-7e03-41f8-a94d-3f720389a68a\",\n",
    "    \"params\":      {\"resource_id\": \"f3354a37-7e03-41f8-a94d-3f720389a68a\", \"limit\": 10000},\n",
    "}\n",
    "\n",
    "# ─── Date columns to parse ───\n",
    "DATE_COLS = {\n",
    "    \"san_jose\": [\"IntakeDate\", \"OutcomeDate\"],\n",
    "    \"dallas\":   [\"Intake_Date\", \"Outcome_Date\"],\n",
    "    \"soco\":     [\"Intake Date\", \"Outcome Date\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## 3. Data Loading\n",
    "\n",
    "**Purpose:**  \n",
    "Pull raw data into pandas DataFrames:  \n",
    "1. Call the San Jose API and parse its JSON response.  \n",
    "2. Read the Dallas + SoCo CSV files, converting date strings to `datetime64`.  \n",
    "\n",
    "This cell remains focused on *how* to load, not *where* (that’s in Configuration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Data Loading ───\n",
    "\n",
    "# San Jose API\n",
    "resp = requests.get(\n",
    "    SAN_JOSE[\"base_url\"], \n",
    "    params=SAN_JOSE[\"params\"]\n",
    "    )\n",
    "resp.raise_for_status()\n",
    "san_jose_df = pd.DataFrame(resp.json()[\"result\"][\"records\"])\n",
    "# Parse as datetimes\n",
    "for col in DATE_COLS[\"san_jose\"]:\n",
    "    san_jose_df[col] = pd.to_datetime(san_jose_df[col], errors=\"coerce\")\n",
    "\n",
    "# Dallas and SoCo CSVs\n",
    "dallas_df = pd.read_csv(\n",
    "    CSV_PATHS[\"dallas\"],\n",
    "    parse_dates=DATE_COLS[\"dallas\"],\n",
    "    low_memory=False\n",
    ")\n",
    "soco_df = pd.read_csv(\n",
    "    CSV_PATHS[\"soco\"],\n",
    "    parse_dates=DATE_COLS[\"soco\"],\n",
    "    low_memory=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## 4. Data Cleaning & Standardization\n",
    "\n",
    "**Purpose:**  \n",
    "Clean up and harmonize column names across sources:  \n",
    "- Apply a single `COLUMN_MAP` dict.  \n",
    "- Lowercase everything for consistency.  \n",
    "- Drop unintended duplicates.  \n",
    "\n",
    "This ensures downstream steps can assume a uniform schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "San Jose columns: ['_id', 'animal_id', 'animalname', 'animal_type', 'primary_color', 'secondarycolor', 'breed', 'sex', 'dob', 'age', 'intake_date', 'intake_condition', 'intake_type', 'intakesubtype', 'intake_reason', 'outcome_date', 'outcome_type', 'outcomesubtype', 'outcomecondition', 'crossing', 'jurisdiction', 'lastupdate']\n",
      "Dallas   columns: ['animal_id', 'animal_type', 'breed', 'kennel_number', 'kennel_status', 'tag_type', 'activity_number', 'activity_sequence', 'source_id', 'census_tract', 'council_district', 'intake_type', 'intake_subtype', 'intake_total', 'reason', 'staff_id', 'intake_date', 'intake_time', 'due_out', 'intake_condition', 'hold_request', 'outcome_type', 'outcome_subtype', 'outcome_date', 'outcome_time', 'receipt_number', 'impound_number', 'service_request_number', 'outcome_condition', 'chip_status', 'animal_origin', 'additional_information', 'month', 'year']\n",
      "SoCo     columns: ['name', 'animal_type', 'breed', 'primary_color', 'sex', 'size', 'date of birth', 'impound number', 'kennel number', 'animal_id', 'intake_date', 'outcome_date', 'days in shelter', 'intake_type', 'intake subtype', 'outcome_type', 'outcome subtype', 'intake_condition', 'outcome condition', 'intake jurisdiction', 'outcome jurisdiction', 'outcome zip code', 'location', 'count']\n"
     ]
    }
   ],
   "source": [
    "# ─── Data Cleaning ───\n",
    "\n",
    "# Here we will define the full column mapping:\n",
    "COLUMN_MAP = {\n",
    "    # Animal ID\n",
    "    \"AnimalID\":      \"animal_id\",\n",
    "    \"Animal_Id\":     \"animal_id\",\n",
    "    \"Animal ID\":     \"animal_id\",\n",
    "    # Animal Type\n",
    "    \"AnimalType\":    \"animal_type\",\n",
    "    \"Animal_Type\":   \"animal_type\",\n",
    "    \"Type\":          \"animal_type\",\n",
    "    # Breed\n",
    "    \"PrimaryBreed\":  \"breed\",\n",
    "    \"Animal_Breed\":  \"breed\",\n",
    "    \"Breed\":         \"breed\",\n",
    "    # Color\n",
    "    \"PrimaryColor\":  \"primary_color\",\n",
    "    \"Color\":         \"primary_color\",\n",
    "    # Age\n",
    "    \"Age\":           \"age\",\n",
    "    # Sex\n",
    "    \"Sex\":           \"sex\",\n",
    "    # Intake fields\n",
    "    \"IntakeType\":        \"intake_type\",\n",
    "    \"Intake_type\":       \"intake_type\",\n",
    "    \"Intake Type\":       \"intake_type\",\n",
    "    \"IntakeCondition\":   \"intake_condition\",\n",
    "    \"Intake_Condition\":  \"intake_condition\",\n",
    "    \"Intake Condition\":  \"intake_condition\",\n",
    "    \"IntakeReason\":      \"intake_reason\",\n",
    "    \"reason\":            \"intake_reason\",\n",
    "    \"IntakeDate\":        \"intake_date\",\n",
    "    \"Intake_Date\":       \"intake_date\",\n",
    "    \"Intake Date\":       \"intake_date\",\n",
    "    # Outcome fields\n",
    "    \"OutcomeType\":       \"outcome_type\",\n",
    "    \"outcome_type\":      \"outcome_type\",\n",
    "    \"Outcome Type\":      \"outcome_type\",\n",
    "    \"OutcomeDate\":       \"outcome_date\",\n",
    "    \"Outcome_Date\":      \"outcome_date\",\n",
    "    \"Outcome Date\":      \"outcome_date\",\n",
    "}\n",
    "\n",
    "# Function to apply the column mapping \n",
    "def standardize_columns(df: pd.DataFrame, mapping: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Standardize DataFrame column names.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The raw DataFrame whose columns need standardization to enable better\n",
    "        analysis.\n",
    "    mapping : dict\n",
    "        A dict where keys are original column names (exact match) and\n",
    "        values are the desired standardized names (snake_case).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A copy of `df` with:\n",
    "        1. Columns renamed according to `mapping`.\n",
    "        2. All column names converted to lowercase.\n",
    "        3. Any duplicate column names (arising when multiple originals map\n",
    "           to the same new name) removed—only the first occurrence is kept.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Columns not present in `mapping` are left unchanged (apart from lowercasing).\n",
    "    - Renaming happens before lowercasing, so mapping keys are case-sensitive.\n",
    "    - Dropping duplicate columns avoids collisions in downstream code.\n",
    "    \"\"\"\n",
    "    # Apply the renaming mapping\n",
    "    df = df.rename(columns=mapping)\n",
    "    # Convert all column names to lowercase\n",
    "    df.columns = df.columns.str.lower()\n",
    "    # Remove duplicate columns, keeping the first occurrence\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "    return df\n",
    "\n",
    "\n",
    "# Apply the column mapping to each DataFrame\n",
    "san_jose_clean = standardize_columns(san_jose_df, COLUMN_MAP)\n",
    "dallas_clean = standardize_columns(dallas_df, COLUMN_MAP)\n",
    "soco_clean = standardize_columns(soco_df, COLUMN_MAP)\n",
    "\n",
    "# Uncomment the following lines to do a quick check\n",
    "# --------------------------------------------------------------\n",
    "# print(\"San Jose columns:\", san_jose_clean.columns.tolist())\n",
    "# print(\"Dallas   columns:\", dallas_clean.columns.tolist())\n",
    "# print(\"SoCo     columns:\", soco_clean.columns.tolist())\n",
    "# --------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## 5. Data Merging & Harmonization\n",
    "\n",
    "**Purpose:**  \n",
    "Combine the cleaned DataFrames into one Bronze-layer table:  \n",
    "- Tag each row with its source (`san_jose`, `dallas`, or `soco`).  \n",
    "- Reindex to a common `FINAL_COLUMNS` list.  \n",
    "- Drop exact duplicates and enforce correct dtypes.  \n",
    "\n",
    "Result: a single `bronze_df` ready for analysis or Silver-layer transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Bronze shape: (105597, 13)\n",
      "source\n",
      "dallas      65063\n",
      "soco        30550\n",
      "san_jose     9984\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# # ─── Data Merging ───\n",
    "\n",
    "# Tag each DataFrame with its source to allow for tracking\n",
    "san_jose_clean[\"source\"] = \"san_jose\"\n",
    "dallas_clean[\"source\"] = \"dallas\"\n",
    "soco_clean[\"source\"] = \"soco\"\n",
    "\n",
    "# Now lets define the final column order for the Bronze Layer\n",
    "FINAL_COLUMNS = [\n",
    "    \"animal_id\",\n",
    "    \"animal_type\",\n",
    "    \"breed\",\n",
    "    \"primary_color\",\n",
    "    \"age\",\n",
    "    \"sex\",\n",
    "    \"intake_type\",\n",
    "    \"intake_condition\",\n",
    "    \"intake_reason\",\n",
    "    \"intake_date\",\n",
    "    \"outcome_type\",\n",
    "    \"outcome_date\",\n",
    "    \"source\",\n",
    "]\n",
    "\n",
    "# This is where we will concatenate the DataFrames\n",
    "def merge_bronze(*dfs: pd.DataFrame, final_cols: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Stack multiple source DataFrames into a single Bronze DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dfs : one or more pandas.DataFrame\n",
    "        Cleaned DataFrames to merge (must share standardized column names).\n",
    "    final_cols : list of str\n",
    "        Desired column order for the merged Bronze layer.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        The combined Bronze DataFrame with only `final_cols`, in that order.\n",
    "    \"\"\"\n",
    "    combined = pd.concat(dfs, ignore_index=True, sort=False)\n",
    "    # Keep only the columns we care about in the specified order\n",
    "    combined = combined.reindex(columns=final_cols)\n",
    "    return combined\n",
    "\n",
    "bronze_df = merge_bronze(\n",
    "    san_jose_clean,\n",
    "    dallas_clean,\n",
    "    soco_clean,\n",
    "    final_cols=FINAL_COLUMNS\n",
    ")\n",
    "\n",
    "#  Drop exact duplicates to make sure we have unique records\n",
    "#  across all sources\n",
    "bronze_df = bronze_df.drop_duplicates()\n",
    "\n",
    "# Display the final DataFrame shape and a count of records by source\n",
    "print(\"Final Bronze shape:\", bronze_df.shape)\n",
    "print(bronze_df[\"source\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## 6. Quick Exploratory Checks\n",
    "\n",
    "**Purpose:**  \n",
    "Perform lightweight diagnostics to understand your data:  \n",
    "- **Missing values:** Where do we need imputation or exclusion?  \n",
    "- **Unique counts:** Which columns are constant vs. highly cardinal?  \n",
    "- **Distributions:** How do key fields like `intake_type` or `outcome_type` break down?  \n",
    "- **Date range:** Are you covering the expected time span?  \n",
    "- **Trends:** Monthly intake counts to spot gaps or seasonality.  \n",
    "\n",
    "Use these insights to guide further cleaning or deeper EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values:\n",
      "animal_id                0\n",
      "animal_type              0\n",
      "breed                   39\n",
      "primary_color        65063\n",
      "age                  95613\n",
      "sex                  65063\n",
      "intake_type              0\n",
      "intake_condition         0\n",
      "intake_reason       105139\n",
      "intake_date              0\n",
      "outcome_type           370\n",
      "outcome_date          1447\n",
      "source                   0\n",
      "dtype: int64 \n",
      "\n",
      "Unique values per column:\n",
      "animal_id           85442\n",
      "animal_type             6\n",
      "breed                1221\n",
      "primary_color         394\n",
      "age                    61\n",
      "sex                    10\n",
      "intake_type            23\n",
      "intake_condition       26\n",
      "intake_reason          25\n",
      "intake_date          3979\n",
      "outcome_type           29\n",
      "outcome_date         3593\n",
      "source                  3\n",
      "dtype: int64 \n",
      "\n",
      "Outcome types (%):\n",
      "outcome_type\n",
      "ADOPTION             28.6%\n",
      "TRANSFER             14.8%\n",
      "FOSTER               11.7%\n",
      "RETURN TO OWNER       9.6%\n",
      "EUTHANIZED            8.2%\n",
      "RETURNED TO OWNER     5.2%\n",
      "EUTHANIZE             4.0%\n",
      "LOST EXP              3.2%\n",
      "TREATMENT             2.1%\n",
      "DISPOSAL              1.9%\n",
      "TNR                   1.8%\n",
      "DIED                  1.4%\n",
      "FOUND EXP             1.4%\n",
      "CLOSED                1.0%\n",
      "RTF                   1.0%\n",
      "OTHER                 0.9%\n",
      "EUTH                  0.7%\n",
      "RTO                   0.7%\n",
      "RESCUE                0.6%\n",
      "WILDLIFE              0.5%\n",
      "FOUND ANIM            0.2%\n",
      "MISSING               0.2%\n",
      "SNR                   0.1%\n",
      "RTOS                  0.1%\n",
      "VET                   0.0%\n",
      "ESCAPED/STOLEN        0.0%\n",
      "REQ EUTH              0.0%\n",
      "NEUTER                0.0%\n",
      "SPAY                  0.0%\n",
      "Name: proportion, dtype: object \n",
      "\n",
      "Intake types (%):\n",
      "intake_type\n",
      "STRAY              57.0%\n",
      "FOSTER             12.0%\n",
      "OWNER SURRENDER    11.9%\n",
      "CONFISCATED         3.8%\n",
      "CONFISCATE          2.8%\n",
      "TNR                 2.2%\n",
      "TREATMENT           2.1%\n",
      "WILDLIFE            1.7%\n",
      "DISPO REQ           1.2%\n",
      "RESOURCE            1.1%\n",
      "QUARANTINE          1.0%\n",
      "KEEPSAFE            0.8%\n",
      "TRANSFER            0.7%\n",
      "ADOPTION RETURN     0.5%\n",
      "S/N CLINIC          0.4%\n",
      "OWNER SUR           0.3%\n",
      "RETURN              0.2%\n",
      "BORN HERE           0.1%\n",
      "DISPOS REQ          0.1%\n",
      "OS APPT             0.0%\n",
      "EUTH REQ            0.0%\n",
      "SPAY                0.0%\n",
      "NEUTER              0.0%\n",
      "Name: proportion, dtype: object \n",
      "\n",
      "Intake date range: 2013-08-16 to 2025-09-27 \n",
      "\n",
      "Age summary:\n",
      "count    0.0\n",
      "mean     NaN\n",
      "std      NaN\n",
      "min      NaN\n",
      "25%      NaN\n",
      "50%      NaN\n",
      "75%      NaN\n",
      "max      NaN\n",
      "Name: age_num, dtype: float64 \n",
      "\n",
      "Monthly intake counts by source:\n",
      "source      dallas  san_jose  soco\n",
      "year_month                        \n",
      "2025-02       2434       153   160\n",
      "2025-03       3017       136   149\n",
      "2025-04       3114        94   275\n",
      "2025-05       1927        50   181\n",
      "2025-09          1         0     0\n"
     ]
    }
   ],
   "source": [
    "# ─── Step 6: Quick Exploratory Checks ───\n",
    "\n",
    "# Missing values per column\n",
    "print(\"Missing values:\")\n",
    "print(bronze_df.isna().sum(), \"\\n\")\n",
    "\n",
    "# Cardinality (distinct counts)\n",
    "print(\"Unique values per column:\")\n",
    "print(bronze_df.nunique(), \"\\n\")\n",
    "\n",
    "# Outcome‐type distribution\n",
    "print(\"Outcome types (%):\")\n",
    "print(bronze_df[\"outcome_type\"]\n",
    "      .value_counts(normalize=True)\n",
    "      .mul(100)\n",
    "      .round(1)\n",
    "      .astype(str) + \"%\", \"\\n\")\n",
    "\n",
    "# Intake‐type distribution\n",
    "print(\"Intake types (%):\")\n",
    "print(bronze_df[\"intake_type\"]\n",
    "      .value_counts(normalize=True)\n",
    "      .mul(100)\n",
    "      .round(1)\n",
    "      .astype(str) + \"%\", \"\\n\")\n",
    "\n",
    "# Time span of intake dates\n",
    "print(\"Intake date range:\",\n",
    "      bronze_df[\"intake_date\"].min().date(),\n",
    "      \"to\",\n",
    "      bronze_df[\"intake_date\"].max().date(),\n",
    "      \"\\n\")\n",
    "\n",
    "# Age summary (if numeric)\n",
    "if \"age\" in bronze_df.columns:\n",
    "    # coerce to numeric (some age values may be strings)\n",
    "    bronze_df[\"age_num\"] = pd.to_numeric(bronze_df[\"age\"], errors=\"coerce\")\n",
    "    print(\"Age summary:\")\n",
    "    print(bronze_df[\"age_num\"].describe(), \"\\n\")\n",
    "\n",
    "#Monthly counts by source (to check seasonality or data gaps)\n",
    "bronze_df[\"year_month\"] = bronze_df[\"intake_date\"].dt.to_period(\"M\")\n",
    "monthly_counts = (\n",
    "    bronze_df\n",
    "    .groupby([\"year_month\", \"source\"])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    ")\n",
    "print(\"Monthly intake counts by source:\")\n",
    "print(monthly_counts.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
